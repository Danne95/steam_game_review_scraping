{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime sample(30): 289 reviews [crawl - 3m32s, parse - 32s] \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import Speller\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "spell = Speller(lang = 'en')\n",
    "number_of_pages_to_scrape = 100             # 10 reviews per page\n",
    "\n",
    "game_name = \"Among Us\"\n",
    "game_steam_number = 945360\n",
    "# Vampire Survivors - 1794680\n",
    "# Elden Ring - 1245620\n",
    "# Among Us - 945360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = \"MySpider\"\n",
    "    download_delay = 6\n",
    "    page_number = 1\n",
    "    start_urls = (\n",
    "    'https://steamcommunity.com/app/{}/reviews/'.format(game_steam_number), \n",
    "    )\n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'LOG_ENABLED': False,\n",
    "        'LOG_FILE': 'logging.txt',\n",
    "        'LOG_FILE_APPEND': False,\n",
    "        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    "        'FEEDS': {\"items.json\": {\"format\": \"json\", 'overwrite': True},},\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(\"hello, page \", self.page_number)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        for review in soup.find_all('div', class_=\"apphub_UserReviewCardContent\"):\n",
    "            new_line = []\n",
    "            \n",
    "            #thumbs_up\n",
    "            if(review.find('div', class_=\"title\").text == \"Recommended\"):\n",
    "                new_line.append(True)\n",
    "            else:\n",
    "                new_line.append(False)\n",
    "            \n",
    "            #text\n",
    "            new_line.append(\"\")\n",
    "            for x in (review.find('div', class_=\"date_posted\").next_siblings):\n",
    "                new_line[-1] += x.text.strip()\n",
    "            \n",
    "            #helpful_votes              \n",
    "            temp = review.find('div', class_=\"found_helpful\").text.strip().split(\" \")[0]\n",
    "            if(temp == \"No\"):\n",
    "                new_line.append(0)\n",
    "            else:\n",
    "                new_line.append(int(temp))\n",
    "            \n",
    "            #funny_votes\n",
    "            if(review.find('div', class_=\"found_helpful\").find('br')):\n",
    "                new_line.append(review.find('div', class_=\"found_helpful\").find('br').next_sibling.text.strip().split(\" \")[0])\n",
    "            else:\n",
    "                new_line.append(0)\n",
    "            \n",
    "            #reward\n",
    "            if(review.find(\"div\", class_=\"reward_btn_icon\")):\n",
    "                new_line.append(int(review.find(\"div\", class_=\"reward_btn_icon\").text))\n",
    "            else:\n",
    "                new_line.append(0)\n",
    "            \n",
    "            #gametime_record\n",
    "            new_line.append(float(review.find(\"div\", class_=\"hours\").text.split(\" \")[0]))\n",
    "            \n",
    "            yield{\n",
    "                'thumbs_up': new_line[0],\n",
    "                'text': new_line[1],\n",
    "                'helpful_votes': new_line[2],\n",
    "                'funny_votes': new_line[3],\n",
    "                'reward': new_line[4],\n",
    "                'gametime_record': new_line[5]\n",
    "            }\n",
    "            \n",
    "        # send Request for more reviews\n",
    "        if(self.page_number<number_of_pages_to_scrape):\n",
    "            form = soup.find('form', id=\"MoreContentForm{}\".format(self.page_number)).find_all()\n",
    "            #form = soup.find('form', id=re.compile(\"^MoreContentForm[0-9]+$\")).find_all()\n",
    "            self.page_number +=1\n",
    "            #yield scrapy.Request('https://steamcommunity.com/app/{gameNum}/homecontent/?userreviewscursor={x}&userreviewsoffset={offset}&p={p}&workshopitemspage={p}&readytouseitemspage={p}&mtxitemspage={p}&itemspage={p}&screenshotspage={p}&videospage={p}&artpage={p}&allguidepage={p}&webguidepage={p}&integratedguidepage={p}&discussionspage={p}&numperpage=10&browsefilter=trendweek&browsefilter=trendweek&l=english&appHubSubSection=10&filterLanguage=default&searchText=&maxInappropriateScore=100'.format(gameNum=game_steam_number, x=form[0]['value'], offset=10*(self.page_number-1) ,p=self.page_number),method='GET', callback=self.parse)\n",
    "            inputs = [game_steam_number] + [x['value'] for x in form][:14]\n",
    "            yield scrapy.Request('https://steamcommunity.com/app/{0}/homecontent/?userreviewscursor={1}&userreviewsoffset={2}&p={3}&workshopitemspage={4}&readytouseitemspage={5}&mtxitemspage={6}&itemspage={7}&screenshotspage={8}&videospage={9}&artpage={10}&allguidepage={11}&webguidepage={12}&integratedguidepage={13}&discussionspage={14}&numperpage=10&browsefilter=trendweek&browsefilter=trendweek&l=english&appHubSubSection=10&filterLanguage=default&searchText=&maxInappropriateScore=100'.format(*inputs),method='GET', callback=self.parse)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text to only \"useful\"\n",
    "def sentence_parse(badText, applyList=[2,1,3]):\n",
    "        funcDict = {}\n",
    "        funcDict[1]=lambda x:[spell(word) for word in x]\n",
    "        funcDict[2]=lambda x:[nltk.stem.SnowballStemmer('english').stem(word) for word in x]\n",
    "        funcDict[3]=lambda x:[nltk.stem.WordNetLemmatizer().lemmatize(word) for word in betterText]\n",
    "        badText = badText.replace(\"[^\\s\\w]\", \"\").lower().encode('ascii', 'ignore').decode('ascii').translate(str.maketrans('', '', string.punctuation))\n",
    "        betterText = badText.split()\n",
    "        betterText = [word for word in betterText if word not in stopWords]\n",
    "        for i in applyList:\n",
    "            betterText = funcDict[i](betterText)\n",
    "        if not betterText:\n",
    "            return None\n",
    "        betterText = \" \".join(i for i in betterText)\n",
    "        return betterText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for the searched game\n",
    "if(not(os.path.exists(game_name))):\n",
    "    os.makedirs(game_name)\n",
    "\n",
    "# add cleaned text to the json and save it in the new folder\n",
    "df = pd.read_json(\"items.json\")\n",
    "df = df.assign(cleaned_text = df['text'].apply(sentence_parse)).dropna()\n",
    "df.to_json(game_name + \"/\" + game_name + \".json\", orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2c0e6a8d3dcd2b1f5288cc57a7946c96b72e9b341f7beb7390001d47b847d81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
